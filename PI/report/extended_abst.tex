\documentclass[a4paper,10pt,openright,openbib,twocolumn]{article}
%\usepackage[portuges]{babel}
\usepackage[T1]{fontenc}
\usepackage{ae}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{url}
\usepackage{listings}
\usepackage{verbatim}
\usepackage{enumerate}
\usepackage[pdftex, bookmarks, colorlinks, linkcolor=black, urlcolor=blue]{hyperref} 
\usepackage[a4paper,left=2.5cm,right=2.5cm,top=3.5cm,bottom=3.5cm]{geometry}
\usepackage{colortbl}
\usepackage[margin=10pt,font=small,labelfont=bf]{caption}
\usepackage{mdwlist}
\usepackage{cleveref}
\usepackage{epsfig}

\usepackage{multicol}
\usepackage{appendix}


\setlength{\parindent}{0cm}
\setlength{\parskip}{2pt}


\begin{document}

\begin{multicols}{2}
\title{Study and Optimization of a Finite Volume Application}
\author{
    Brito, Rui\\
    PG22781\\
    Department of Informatics\\
    University of Minho\\
    ruibrito666@gmail.com
  \and
    Alves, Jos√©\\
    PG22765\\
    Department of Informatics\\
    University of Minho\\
    zealves.080@gmail.com
}
\date{}
\maketitle
\end{multicols}

\section{Introduction}    %% Reler, corrigir algum erro

High-Performance Computing (HPC) has been a fundamental process to science, in order to make viable certain simulations that require massive amounts of calculation. The use of several optimization techniques has helped decrease the execution time of different algorithms, providing critical results faster, thus helping research move forward at a faster pace.

This document will describe a study about improving the performance of \emph{conv-diff}. This application calculates de heat diffusion of, let's say, a liquid while it spreads though an area. In this study we approach different ways to improve uppon the initial solution solution. The project was devised in three stages. The first one was analysing the original application, building its profile while also developing a better sequencial version. 
On the second stage, a shared-memory parallel version was made (\emph{OpenMP}). Finaly on the third stage, a CUDA version is being developed, taking advantage of the massive paralellism modern GPU's have to offer.

This extended abstract presents the aplication's domain, explaining its uses and objectives. By knowing its domain and profile, a string foundation is created to develop all kinds of different optimizations. From here, an optimized sequencial version was developed, as well as an OpenMP shared-memory version. The ongoing and future work and a brief conclusion will be presented in the last sections of this extended abstract.

\section{Case Study}    %% Reler, corrigir algum erro, adicionar algo se te lembrares

The application analyzed for this study is \emph{conv-diff(Convexion-Diffusion)}.This application simulates the way heat is tranfered in a fluid using the finite-volumes method.To compute the heat diffusion, the surface is represented as a mesh. Being represented by cells and edges, the algorithm will traverse all edges, calculating the contribution of the adjacent cells. This application rests in a Finite Volume Library (FVLib), which handles the structures and some of the logic functions necessary for the problem's solution.

The application's main objective is to compute a vector $\overline{\phi}$ such that $\overline{\phi} \longrightarrow G(\overline{\phi}) = \left(\begin{array}{c}
0\\ 
0\\
\vdots\end{array}\right)$
This is acomplished in three different stages:
\begin{enumerate}
    \item {We begin with a candidate vector $\phi$} 
    \item {For each edge, we compute the flux $F_{ij}$, with $i$ and $j$ being the indices of the adjacent cells}
    \item {For each cell, we compute $\sum |e_{ij}| F_{ij} - |c_i| f_i$}
\end{enumerate}
Thus: $\phi = \left(\begin{array}{c}
\phi_1\\
\vdots\\
\phi_I
\end{array}\right) \longrightarrow G = \left(\begin{array}{c}
G_1\\
\vdots\\
G_I
\end{array}\right)$


\section{Profiling}    %% Reler, reestruturar e adicionar o que faltar

The program consists in four major parts, reading the initial mesh from a file. Then, using the functions \emph{makeResidual}, which calls the function \emph{makeFlux}, the flux contributions are calculated and the vector phi is built, thus achieving a matrix free implementation. Following this, to calculate the deviation in the results from the previous operations, the function \emph{LUFactorize}. Finally, both the meshes are writen to the output files, together with the error between them.
 
After analyzing the application, we conclude that the algorithm has a very high workload in the \emph{LUFactorize} function, comprising of more than 90\% of the execution time.


\section{Sequencial Optimization}

After analysing the code and understanding the problem at hand, we began to notice some implementation errors, these errors were very easy to spot, such as reading the same variable repeatedly from a file and a very long chain of calculations with a very heavy division at the end which could clearly have been avoided. We changed all those aspects of the application because they were easy, requiring minimal effort. That being said, it doesn't mean they didn't pay results, computation time has been greatly reduced, with the aforementioned \emph{LUFactorize} function taking an even more prominent role in our profile.

\section{Shared Memory Parallel Optimization(OpenMP)}

After optimizing the sequential code, we turned our efforts to parallelizing the code. The two loops responsible for the matrix free calculations were ideal candidates. We parallelized both this loops. We had some strugles with data-races in these loops, but we overcame these problems rather easily. The data-races exist beacause the mesh is traversed by the edges, however, as we found out, if they are traversed by cell, these data-races no longer exist. Also, the library that was provided includes some iterator style structures. These were also a problem, because, while OpenMP as no problem in parallelizing STL iterators, this doesn't hold for \emph{FVlib}'s iterators. So, we had to convert those to a standard for loop. The code was successfully parallelized, however, results were disappointing, execution time didn't decrease noticeably, hinting at a very memory bound application. 

\section{On-going and Future Work}    %% Reler, corrigir algum erro

After implementing both the sequencial version and the OpenMP version, a na\"{i}ve implementation in CUDA was started. This version aims to take advantage of the graphics board performance using its vector units to diminish the compute time. For this a restructuring of the code is necessary removing most of the memory accesses while maintaining an abstraction of the system. After resolving this issue and other minor ones, we expect to achieve a boost in performance.

In future work, it is expected to develop a optimized CUDA version as well as a OpenMPI. A hybrid version using both CPU and GPU could also be a future implementation to have gains in performance. It is also expected to optimize several areas of code, questioning some decisions like using double-precision versus single-precision. 

\section{Conclusion}

This extended abstract serves as a introduction to the study here presented. The initial results from the implementations of optimized versions, sequential and parallel, shows the capability of improvements in the computed time.

Through the development of the solutions some problems were presented, such as the mesh being disperse and the structures implemented with  extensive use of pointers. This problems delayed the development of solutions, in particular the CUDA version. The decision of maintaining the abstraction of the system may prove to be a bold direction, but favorable in terms of comprehension.

In the future released paper a deep analysis of the results will be made, showing the performance improvements obtained.

\end{document}
